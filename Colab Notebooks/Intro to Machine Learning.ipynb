{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1KmfAuETLrFBL2VsjzRdOqRo_Z7NNNUAP","timestamp":1678416312440}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0Y-HINws7t4z"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# Common AI models\n","\n","In this first notebook you will practice AI with some common Machine Learning models. You're expected to already have some basic experience in playing around with Python. If you find yourself stranded on one of the assignments please put a question in the bootcamp-questions Slack channel. There are mentors there that are eager to help."],"metadata":{"id":"9JixMZox71uW"}},{"cell_type":"markdown","source":["# Classic machine learning models\n","\n","## Assignment 1\n","From the Sklearn library choose models of at least the following types, train them on the 6 imported datasets, evaluate their accuracy or R^2 and see which model works best on which dataset. (Note that there are both regression and classification sets)\n","* Tree\n","* Neural Network\n","* Neighbors\n","* Ensemble\n","* Naive Byes (classification only)\n","* Linear\n"],"metadata":{"id":"BQjIosCd73lX"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris, load_diabetes, load_digits, load_wine, load_breast_cancer\n","from sklearn.metrics import accuracy_score, r2_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Example of how to load a dataset\n","print(load_iris().DESCR)\n","\n","X = load_iris().data\n","y = load_iris().target\n","\n","# Create a train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"],"metadata":{"id":"boxe8FST7zpC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example of how to load/train a model and evaluate\n","from sklearn.tree import DecisionTreeClassifier\n","tree = DecisionTreeClassifier()             # Load\n","fitted_tree = tree.fit(X_train, y_train)    # Train\n","y_pred = fitted_tree.predict(X_test)        # Predict on unseen data\n","\n","acc_score = accuracy_score(y_test, y_pred)  # Evaluate accuracy score\n","r2_score = r2_score(y_test, y_pred)         # Evaluate R^2 score\n","\n","print(f'On the {load_iris.__name__} dataset the {DecisionTreeClassifier.__name__} reaches an accuracy score of {acc_score} and a R^2 score of {r2_score}')"],"metadata":{"id":"MfM5F3WL_GdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_iris, load_diabetes, load_digits, load_wine, load_breast_cancer\n","from sklearn.metrics import accuracy_score, r2_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","dataSets = [load_iris, load_diabetes, load_digits, load_wine, load_breast_cancer]\n","\n","for i in range(len(dataSets)):\n","  scores =[]\n","  module = []\n","  # Conducts the DecisionTree algorithm for the data sets\n","  X = dataSets[i]().data\n","  y = dataSets[i]().target\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","  tree = DecisionTreeClassifier()\n","  fitted_tree = tree.fit(X_train, y_train)\n","  y_pred = fitted_tree.predict(X_test)\n","\n","  acc_score = accuracy_score(y_test, y_pred)\n","  r2 = r2_score(y_test, y_pred)\n","\n","  scores.append(acc_score)\n","  module.append(DecisionTreeClassifier.__name__)\n","\n","  print(f'On the {dataSets[i].__name__} dataset the {DecisionTreeClassifier.__name__} reaches an accuracy score of {acc_score} and a R^2 score of {r2}')\n","\n","\n"," # Conducts the Neural Network algorithm for the data sets\n","\n","  if dataSets[i].__name__ == \"load_iris\":\n","    mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='sgd', max_iter=500, random_state=0, early_stopping=True)\n","  else:\n","    mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='sgd', max_iter=500, random_state=0)\n","\n","  mlp.fit(X_train, y_train)\n","  y_pred = mlp.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","  acc_score = accuracy_score(y_test, y_pred)\n","\n","  print(f'On the {dataSets[i].__name__} dataset the {MLPClassifier.__name__} reaches an accuracy score of {acc_score} and a R^2 score of {r2}')\n","\n","  scores.append(acc_score)\n","  module.append(MLPClassifier.__name__)\n","\n","    # Conducts the Neighbors algorithm for the data sets\n","  knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","  knn.fit(X_train, y_train)\n","\n","  y_pred = knn.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","  acc_score = accuracy_score(y_test, y_pred)\n","\n","  print(f'On the {dataSets[i].__name__} dataset the {KNeighborsClassifier.__name__} reaches an accuracy score of {acc_score} and a R^2 score of {r2}')\n","\n","  scores.append(acc_score)\n","  module.append(KNeighborsClassifier.__name__)\n","\n"," # Conducts the Ensemble algorithm for the data set\n","  X_train, X_test, y_train, y_test, = train_test_split(X, y, test_size=0.2, random_state=42)\n","  rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","  rf.fit(X_train, y_train)\n","\n","  y_pred = rf.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","  acc_score = accuracy_score(y_test, y_pred)\n","\n","  print(f'On the {dataSets[i].__name__} dataset the {RandomForestClassifier.__name__} reaches an accuracy score of {acc_score} and a R^2 score of {r2}')\n","\n","  scores.append(acc_score)\n","  module.append(RandomForestClassifier.__name__)\n","\n"," # Conducts the Naive Byes algorithm for the data set\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","  nb = GaussianNB()\n","  nb.fit(X_train, y_train)\n","\n","  y_pred = nb.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","  acc_score = accuracy_score(y_test, y_pred)\n","\n","  print(f'On the {dataSets[i].__name__} dataset the {GaussianNB.__name__} reaches an accuracy score of {acc_score} and a R^2 score of {r2}')\n","\n","  scores.append(acc_score)\n","  module.append(GaussianNB.__name__)\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  lr = LogisticRegression(max_iter=10000)\n","  lr.fit(X_train, y_train)\n","\n","  y_pred = lr.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","  acc_score = accuracy_score(y_test, y_pred)\n","\n","  print(f'On the {dataSets[i].__name__} dataset the {LogisticRegression.__name__} reaches an accuracy score of {acc_score} and a R^2 score of {r2}')\n","\n","  scores.append(acc_score)\n","  module.append(LogisticRegression.__name__)\n","  \n","  topScore = min(scores, key=lambda x: abs(x-1.0))\n","  index = scores.index(topScore)\n","  topModule = module[index]\n","\n","  print()\n","  print(f'The {topModule} module worked best for the {dataSets[i].__name__} dataset with a accuracy score of {topScore}')\n","  print()\n","\n","\n","\n","\n","\n"],"metadata":{"id":"RYu_CAk6a0rG","executionInfo":{"status":"ok","timestamp":1678518381659,"user_tz":480,"elapsed":8629,"user":{"displayName":"Calum Crawford","userId":"06110382405882914570"}},"outputId":"0052b096-eb75-4530-b0ba-545a6c9becb7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["On the load_iris dataset the DecisionTreeClassifier reaches an accuracy score of 0.98 and a R^2 score of 0.9712808730614589\n","On the load_iris dataset the MLPClassifier reaches an accuracy score of 0.52 and a R^2 score of 0.3107409534750145\n","On the load_iris dataset the KNeighborsClassifier reaches an accuracy score of 1.0 and a R^2 score of 1.0\n","On the load_iris dataset the RandomForestClassifier reaches an accuracy score of 1.0 and a R^2 score of 1.0\n","On the load_iris dataset the GaussianNB reaches an accuracy score of 1.0 and a R^2 score of 1.0\n","On the load_iris dataset the LogisticRegression reaches an accuracy score of 1.0 and a R^2 score of 1.0\n","\n","The KNeighborsClassifier module worked best for the load_iris dataset with a accuracy score of 1.0\n","\n","On the load_diabetes dataset the DecisionTreeClassifier reaches an accuracy score of 0.00684931506849315 and a R^2 score of -0.07747773215197507\n","On the load_diabetes dataset the MLPClassifier reaches an accuracy score of 0.0 and a R^2 score of -1.9730445083075265\n","On the load_diabetes dataset the KNeighborsClassifier reaches an accuracy score of 0.0 and a R^2 score of -0.14033830664627578\n","On the load_diabetes dataset the RandomForestClassifier reaches an accuracy score of 0.0 and a R^2 score of 0.17343021817795368\n","On the load_diabetes dataset the GaussianNB reaches an accuracy score of 0.0 and a R^2 score of 0.0528242419101258\n","On the load_diabetes dataset the LogisticRegression reaches an accuracy score of 0.0 and a R^2 score of -0.07431994826369315\n","\n","The DecisionTreeClassifier module worked best for the load_diabetes dataset with a accuracy score of 0.00684931506849315\n","\n","On the load_digits dataset the DecisionTreeClassifier reaches an accuracy score of 0.8501683501683501 and a R^2 score of 0.7429044801162432\n","On the load_digits dataset the MLPClassifier reaches an accuracy score of 0.8754208754208754 and a R^2 score of 0.7844320843831812\n","On the load_digits dataset the KNeighborsClassifier reaches an accuracy score of 0.9833333333333333 and a R^2 score of 0.9528923366140218\n","On the load_digits dataset the RandomForestClassifier reaches an accuracy score of 0.9722222222222222 and a R^2 score of 0.9510943341947097\n","On the load_digits dataset the GaussianNB reaches an accuracy score of 0.8472222222222222 and a R^2 score of 0.5814250367841327\n","On the load_digits dataset the LogisticRegression reaches an accuracy score of 0.9722222222222222 and a R^2 score of 0.9676359564523814\n","\n","The KNeighborsClassifier module worked best for the load_digits dataset with a accuracy score of 0.9833333333333333\n","\n","On the load_wine dataset the DecisionTreeClassifier reaches an accuracy score of 0.9491525423728814 and a R^2 score of 0.8264705882352941\n","On the load_wine dataset the MLPClassifier reaches an accuracy score of 0.3389830508474576 and a R^2 score of -1.429411764705883\n","On the load_wine dataset the KNeighborsClassifier reaches an accuracy score of 0.8055555555555556 and a R^2 score of 0.2380952380952378\n","On the load_wine dataset the RandomForestClassifier reaches an accuracy score of 1.0 and a R^2 score of 1.0\n","On the load_wine dataset the GaussianNB reaches an accuracy score of 1.0 and a R^2 score of 1.0\n","On the load_wine dataset the LogisticRegression reaches an accuracy score of 1.0 and a R^2 score of 1.0\n","\n","The RandomForestClassifier module worked best for the load_wine dataset with a accuracy score of 1.0\n","\n","On the load_breast_cancer dataset the DecisionTreeClassifier reaches an accuracy score of 0.9148936170212766 and a R^2 score of 0.6289626248920686\n","On the load_breast_cancer dataset the MLPClassifier reaches an accuracy score of 0.6436170212765957 and a R^2 score of -0.5537190082644627\n","On the load_breast_cancer dataset the KNeighborsClassifier reaches an accuracy score of 0.9298245614035088 and a R^2 score of 0.7012774320340649\n","On the load_breast_cancer dataset the RandomForestClassifier reaches an accuracy score of 0.9649122807017544 and a R^2 score of 0.8506387160170324\n","On the load_breast_cancer dataset the GaussianNB reaches an accuracy score of 0.9736842105263158 and a R^2 score of 0.8879790370127744\n","On the load_breast_cancer dataset the LogisticRegression reaches an accuracy score of 0.956140350877193 and a R^2 score of 0.8132983950212905\n","\n","The GaussianNB module worked best for the load_breast_cancer dataset with a accuracy score of 0.9736842105263158\n","\n"]}]},{"cell_type":"markdown","source":["## Assignment 2\n","Use XGBoost running on GPU to predict the same datasets. You can activate GPU acceleration in the Runtime tab:\n","Runtime -> Change runtime type -> Select GPU from the dropdown"],"metadata":{"id":"2lkpnT2YJIgZ"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris, load_diabetes, load_digits, load_wine, load_breast_cancer\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBRegressor\n","\n","dataSets = [load_iris, load_diabetes, load_digits, load_wine, load_breast_cancer]\n","\n","for i in range(len(dataSets)):\n","\n","  X = dataSets[i]().data\n","  y = dataSets[i]().target\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  model = XGBRegressor()\n","\n","  model.fit(X_train, y_train)\n","\n","  y_pred = model.predict(X_test)\n","  r2 = r2_score(y_test, y_pred)\n","\n","  print(f'On the {dataSets[i].__name__} dataset {XGBRegressor.__name__} module reaches an R^2 score of {r2}')\n","\n","\n","\n","\n","\n"],"metadata":{"id":"48DO3Vk4Mq12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678518191329,"user_tz":480,"elapsed":1981,"user":{"displayName":"Calum Crawford","userId":"06110382405882914570"}},"outputId":"7eab46f0-f1d6-4a1f-e96d-a05fcb512b44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On the load_iris dataset XGBRegressor module reaches an R^2 score of 0.9926678928287365\n","On the load_diabetes dataset XGBRegressor module reaches an R^2 score of 0.22857599305390852\n","On the load_digits dataset XGBRegressor module reaches an R^2 score of 0.8497827487365098\n","On the load_wine dataset XGBRegressor module reaches an R^2 score of 0.702832079593972\n","On the load_breast_cancer dataset XGBRegressor module reaches an R^2 score of 0.811670892353153\n"]}]}]}